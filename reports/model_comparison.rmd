---
title: "Model Comparison"
output: 
  html_document:
    toc: false
    toc_depth: 2
    toc_float: false
    number_sections: false
    df_print: paged
---

```{r, setup, echo = FALSE, results = "hide", message = FALSE}

library(here)
library(tidyverse)
library(patchwork)

source(here("code", "inferential", "_plotting.R"))
source(here("code", "_constants.R"))

input_name <- "Schaefer400_stats.rds"
test_session <- "baseline"
responses <- c("uv", "rda")  # The first one is used as the base level
models <- c("no_lscov_symm")

dat <- readRDS(here("out", "inferential", atlas_nm, input_name))
dat <- bind_rows(dat, .id = "model__response__session") %>%
  separate(model__response__session, c("model", "response", "session"), sep = "__") %>%
  filter(model %in% .env$models, response %in% .env$responses, session %in% .env$test_session) %>%
  mutate(model = factor(model, .env$models, ordered = T),
    response = factor(response, .env$responses, ordered = T),
    is_core32 = region %in% rois[core32]) %>%
  arrange(model, response, region)
```

# Model Comparison

## Fitting Statistics

First we check the outliers in the data by the fitting statistics:

```{r, outliers, eval = TRUE, echo = FALSE, fig.dim = c(16, 9)}
fit_stat <- dat %>%
  filter(Term %in% c("Bayes_R2", "elpd_loo")) %>%
  select(region, model, response, Term, Estimate, is_core32) %>%
  pivot_wider(values_from = Estimate, names_from = Term)
for (i in seq_along(responses)) {
  res <- responses[[i]]
  fig <- ggplot(fit_stat %>% filter(response == res)) +
    aes(x = Bayes_R2, y = elpd_loo, color = is_core32) +
    scale_color_manual(values = c("TRUE" = "red", "FALSE" = "grey")) +
    geom_point() +
    geom_rug() +
    labs(title = res)
  if (i == 1) figs <- fig else figs <- figs + fig
}
figs + plot_annotation(title = "Fitting statistics for Core32 ROIs", tag_levels = "A")
```

There seems to be one outlier for rda, so we exclude it from the future analysis. We will fit another model for it again.

```{r, exclusion, echo = TRUE, eval = TRUE}
outlier_dat <- dat %>% filter(Term == "elpd_loo", Estimate < -60000)
outlier_dat
outliers <- outlier_dat$region
dat <- dat %>% filter(!region %in% .env$outliers)
```

~~Then we compare the Bayesian R-squared of the models (see [here](https://paul-buerkner.github.io/brms/reference/bayes_R2.brmsfit.html)). Each line is for one ROI. The higher the value, the better the fit.~~

```{r, Bayes-R2, echo = FALSE, message = FALSE, fig.dim = c(16, 12), eval=FALSE}

tmp <- dat %>%
  filter(Term == "Bayes_R2") %>%
  group_by(model) %>%
  summarize(mean_R2 = mean(Estimate)) %>%
  arrange(-mean_R2)
tmp

for (i in seq_along(responses)) {
  res <- responses[[i]]
  fig <- ggplot(dat %>% filter(Term == "Bayes_R2", response == .env$res),
    aes(x = model, y = Estimate, group = region)) +
    geom_line(linetype = 1, color = "black", size = 0.5) + geom_point(size = 0.5) +
    labs(x = NULL, y = NULL, title = res) + ylim(0, 0.05) +
    theme(axis.text.x = element_text(size = 14, hjust = 1, vjust = 1, angle = 25))
  if (i == 1) figs <- fig + labs(y = "Bayes R2") else figs <- figs + fig
}
figs + plot_annotation(title = "Bayesian R-Squared for Core32 ROIs", tag_levels = "A")
```

~~Next we show the difference in the expected log predictive density (elpd) estimated by leave-one-out (loo) cross-validation across models (see [here](https://mc-stan.org/loo/reference/loo.html)). The higher the value, the better the fit. Here "`r models[[1]]`" is used as the base level.~~

```{r, diff-elpd-loo, echo = FALSE, message = FALSE, warning = TRUE, fig.dim = c(16, 12), eval=FALSE}

# Get data
diff_dat <- get_diff_dat(dat %>% filter(Term == "elpd_loo"), name_term = "model",
  id_term = c("region", "response"))

# Print the range of the difference in ELPD_LOO
tmp <- diff_dat %>%
  pivot_wider(names_from = model, values_from = Estimate) %>%
  summarise(across(contains(" - "), ~ quantile(.x, probs = c(0, 0.05, 0.25, 0.5, 0.75, 0.95, 1)))) %>%
  t()
colnames(tmp) <- paste0("Q", c(0, 5, 25, 50, 75, 95, 100))
print("Range of the difference in EPLD_LOO between different models:")
print(tmp)

ylims <- c(-10, 30)
for (i in seq_along(responses)) {
  res <- responses[[i]]
  fig <- ggplot(diff_dat %>% filter(response == .env$res),
    aes(x = model, y = Estimate, group = region)) +
    geom_line(linetype = 1, color = "black") + geom_point() +
    labs(x = NULL, y = NULL, title = res) + ylim(ylims[[1]], ylims[[2]]) +
    theme(axis.text.x = element_text(size = 14, hjust = 1, vjust = 1, angle = 25))
  if (i == 1) figs <- fig + labs(y = "Difference between EPLD_LOO") else figs <- figs + fig
}
figs + plot_annotation(title = paste("Difference in elpd_loo between",
  models[[1]], "and other models for Core32 ROIs"), tag_levels = "A")
```

~~We can see that "no_lscov_symm" is the best and "fixed_sigma" is much worse than others. Results using [waic](https://mc-stan.org/loo/reference/waic.html) instead of `loo` are almost identical.~~

```{r, elpd-waic, echo = FALSE, message = FALSE, fig.dim = c(16, 12), eval = FALSE}
for (i in seq_along(responses)) {
  res <- responses[[i]]
  fig <- ggplot(dat %>% filter(response == .env$res, Term == "elpd_waic"),
    aes(x = model, y = Estimate, group = region)) +
    geom_line(linetype = 1, color = "black") + geom_point() +
    labs(x = NULL, y = NULL, title = res) +
    theme(axis.text.x = element_text(size = 14, hjust = 1, vjust = 1, angle = 25))
  if (i == 1) figs <- fig + labs(y = "elpd_waic") else figs <- figs + fig
}
figs + plot_annotation(title = "elpd_waic for Core32 ROIs",
  tag_levels = "A")
```

## Test-Retest Reliability

### Distribution Plots

Here we plot the distribution of TRR over 32 ROIs for each response type and model:

```{r, TRR-hist, echo = FALSE, message = FALSE, fig.dim = c(16, 9)}
ylims <- range(dat %>%
  filter(Term == "TRR") %>%
  select(Estimate)
)
ylims <- ylims + c(-0.1, 0.1) * (ylims[[2]] - ylims[[1]])

for (i in seq_along(models)) {
  mdl <- models[[i]]
  fig <- ggplot(dat %>% filter(Term == "TRR", model == .env$mdl),
    aes(x = response, y = Estimate, fill = response)) +
    geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
    geom_jitter(size = 0.02, height = 0, width = 0.1, aes(color = is_core32)) +
    scale_color_manual(values = c("TRUE" = "red", "FALSE" = "grey")) +
    ylim(ylims[[1]], ylims[[2]]) + labs(x = mdl, y = NULL) +
    theme(axis.text.x = element_blank())
  if (i == 1) figs <- fig + labs(y = "TRR") else figs <- figs + fig
}
figs + plot_layout(nrow = 1, guides = "collect") +
  plot_annotation(title = "Test-Retest Reliability over ROIs", tag_levels = "A")
```

Then we plot the distribution of the difference in TRR between different response types for each model.

```{r, TRR-res-diff-hist, echo = FALSE, message = FALSE, fig.dim = c(16, 9)}
res_diff_dat <- dat %>%
  filter(Term == "TRR") %>%
  get_diff_dat(name_term = "response", id_term = c("region", "model", "is_core32"))
ylims <- range(res_diff_dat$Estimate)
ylims <- ylims + c(-0.1, 0.1) * (ylims[[2]] - ylims[[1]])
for (i in seq_along(models)) {
  mdl <- models[[i]]
  fig <- ggplot(res_diff_dat %>% filter(model == .env$mdl),
    aes(x = response, y = Estimate, fill = response)) +
    geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
    geom_jitter(size = 0.02, height = 0, width = 0.1, aes(color = is_core32)) +
    scale_color_manual(values = c("TRUE" = "red", "FALSE" = "grey")) +
    ylim(ylims[[1]], ylims[[2]]) + labs(x = mdl, y = NULL) +
    theme(axis.text.x = element_blank())
  if (i == 1) figs <- fig + labs(y = "TRR") else figs <- figs + fig
}
figs + plot_layout(nrow = 1, guides = "collect") +
  plot_annotation(title = "Test-Retest Reliability over ROIs", tag_levels = "A")
```

~~This is a similar plot but for the difference between models for each response type:~~

```{r, TRR-mdl-diff-hist, echo = FALSE, message = FALSE, fig.dim = c(16, 9), eval=FALSE}
mdl_diff_dat <- dat %>%
  filter(Term == "TRR") %>%
  get_diff_dat(name_term = "model", id_term = c("region", "response"))
ylims <- range(mdl_diff_dat$Estimate)
ylims <- ylims + c(-0.1, 0.1) * (ylims[[2]] - ylims[[1]])
for (i in seq_along(responses)) {
  res <- responses[[i]]
  fig <- ggplot(mdl_diff_dat %>% filter(response == .env$res),
    aes(x = model, y = Estimate, fill = model)) +
    geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
    geom_jitter(size = 0.02, height = 0, width = 0.1) +
    ylim(ylims[[1]], ylims[[2]]) + labs(x = res, y = NULL) +
    theme(axis.text.x = element_blank())
  if (i == 1) figs <- fig + labs(y = "TRR") else figs <- figs + fig
}
figs + plot_layout(nrow = 1, guides = "collect") +
  plot_annotation(title = "Test-Retest Reliability over ROIs", tag_levels = "A")
```

### Brain Plots

Here we compare the test-retest reliability (TRR) of high-low control demand contrast among response variables and models:

```{r, TRR-brain, echo = FALSE, message = FALSE, fig.dim = c(16, 9)}

ylims <- range(dat %>%
  filter(Term == "TRR") %>%
  select(Estimate)
)
ylims <- ylims + c(-0.1, 0.1) * (ylims[[2]] - ylims[[1]])

figs <- NULL
for (i in seq_along(responses)) {
  for (j in seq_along(models)) {
    res <- responses[[i]]
    mdl <- models[[j]]
    fig <- brain_plot(
      dat %>% filter(Term == "TRR", response == .env$res, model == .env$mdl),
      lim = ylims, fig_title = paste(res, mdl, sep = ", ")
    )
    if (is.null(figs)) figs <- fig else figs <- figs + fig
  }
}
figs <- (figs + plot_layout(nrow = length(responses)) +
  plot_annotation(title = "Test-Retest Reliability", tag_levels = "A"))
print(figs)
```

Here we plot the difference in TRR between different types of response:

```{r, TRR-res-diff-brain, echo = FALSE, message = FALSE, fig.dim = c(16, 9)}

ylims <- range(res_diff_dat$Estimate)
ylims <- ylims + c(-0.1, 0.1) * (ylims[[2]] - ylims[[1]])

figs <- NULL
for (i in seq_along(responses)) {
  if (i == 1) next
  for (j in seq_along(models)) {
    res <- responses[[i]]
    mdl <- models[[j]]
    term_name <- paste(res, "-", responses[[1]])
    fig <- brain_plot(res_diff_dat %>%
      filter(model == .env$mdl, response == .env$term_name),
      lim = ylims,
      fig_title = paste0(term_name, ", ", mdl))
    if (is.null(figs)) figs <- fig else figs <- figs + fig
  }
}
figs <- (figs + plot_layout(nrow = length(responses) - 1) +
  plot_annotation(title = "Difference in TRR between responses", tag_levels = "A"))
print(figs)
```

~~and between different models:~~

```{r, TRR-mdl-diff-brain, echo = FALSE, message = FALSE, fig.dim = c(16, 9), eval=FALSE}

ylims <- range(mdl_diff_dat$Estimate)
ylims <- ylims + c(-0.1, 0.1) * (ylims[[2]] - ylims[[1]])

figs <- NULL
for (i in seq_along(responses)) {
  for (j in seq_along(models)) {
    if (j == 1) next
    res <- responses[[i]]
    mdl <- models[[j]]
    term_name <- paste(mdl, "-", models[[1]])
    fig <- brain_plot(mdl_diff_dat %>%
      filter(response == .env$res, model == .env$term_name),
      lim = ylims,
      fig_title = paste0(res, ", ", term_name))
    if (is.null(figs)) figs <- fig else figs <- figs + fig
  }
}
figs <- (figs + plot_layout(nrow = length(responses)) +
  plot_annotation(title = "Difference in TRR between models", tag_levels = "A"))
print(figs)
```

## Comparison with Previous Results (Skipped)

Here we plot the TRR over core32 parcels and compare it with our previous results. The old method is different from the current one in the following ways:

- Subjects: only 18 instead of 27
- Preprocessing: no divisive normalization for each session
- Response variables:
  - "ridge" uses `mda::fda()`, which is similar to `"rda"` but implemented using ridge regression
  - "rda_full" and "rda_diag" use `sparsediscrim::lda_schafer()` instead of `klaR` to generate the response variable, with either the full covariance matrix (including noise correlation) or only its diagonal (the signal).
  - "uv" is not demeaned for each run
- The TRR model is similar to "fix_sigma" model except that:
  - The model is Gaussian instead of t-distributed
  - The fixed effects are treatment-coded

```{r, TRR-hist-core32, echo = FALSE, message = FALSE, fig.dim = c(21, 9), eval=FALSE}

# Read fixed_std model stats
full_models <- c(models, "gaussian")
full_responses <- c("uv", "lda_diag", "lda_full", "rda", "ridge")
old_dat <- readRDS(here("out", "spatial", "archive", "fix_std_mdl_core32_stats.rds"))
old_dat <- bind_rows(old_dat, .id = "model__response__session") %>%
  separate(model__response__session, c("model", "response", "session"), sep = "__") %>%
  filter(Term == "TRR") %>%
  filter(!region %in% .env$outliers) %>%
  mutate(model = "gaussian")
full_dat <- bind_rows(dat, old_dat) %>%
  mutate(model = factor(model, .env$full_models, ordered = T),
    response = factor(response, .env$full_responses, ordered = T)) %>%
  arrange(model, response, region)
old_dat <- full_dat %>% filter(model == "gaussian")

ylims <- range(full_dat %>%
  filter(Term == "TRR") %>%
  select(Estimate)
)
ylims <- ylims + c(-0.1, 0.1) * (ylims[[2]] - ylims[[1]])
for (i in seq_along(full_models)) {
  mdl <- full_models[[i]]
  fig <- ggplot(full_dat %>% filter(Term == "TRR", model == .env$mdl),
    aes(x = response, y = Estimate, fill = response)) +
    geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
    geom_jitter(size = 0.02, height = 0, width = 0.1) +
    ylim(ylims) + labs(x = mdl, y = NULL) +
    theme(axis.text.x = element_blank())
  if (i == 1) figs <- fig + labs(y = "TRR") else figs <- figs + fig
}
figs + plot_layout(nrow = 1, guides = "auto") +
  plot_annotation(title = "Test-Retest Reliability over ROIs", tag_levels = "A")
```

And the distribution of the difference between response types:

```{r, TRR-diff-hist-core32, echo = FALSE, message = FALSE, fig.dim = c(21, 9), eval=FALSE}

res_diff_old <- get_diff_dat(old_dat, "response", id_term = c("region", "model"))
res_diff_full <- bind_rows(
  res_diff_dat %>% mutate(model = factor(model, .env$full_models)),
  res_diff_old %>% mutate(model = factor(model, .env$full_models))
)

ylims <- range(res_diff_full$Estimate)
ylims <- ylims + c(-0.1, 0.1) * (ylims[[2]] - ylims[[1]])
for (i in seq_along(full_models)) {
  mdl <- full_models[[i]]
  fig <- ggplot(res_diff_full %>% filter(model == .env$mdl),
    aes(x = response, y = Estimate, fill = response)) +
    geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
    geom_jitter(size = 0.02, height = 0, width = 0.1) +
    ylim(-1, 1) + labs(x = mdl, y = NULL) +
    theme(axis.text.x = element_blank())
  if (i == 1) figs <- fig + labs(y = "TRR Difference") else figs <- figs + fig
}
figs + plot_layout(nrow = 1, guides = "auto") +
  plot_annotation(title = "Difference of TRR between response variables over ROIs", tag_levels = "A")
```