---
title: "Model Comparison"
output: 
  html_document:
    toc: false
    toc_depth: 2
    toc_float: false
    number_sections: false
    df_print: paged
---

```{r, setup, echo = FALSE, results = "hide", message = FALSE}

library(here)
library(tidyverse)
library(patchwork)

source(here("code", "inferential", "_plotting.R"))

input_name <- "core32_stats.rds"
test_session <- "baseline"
mdl_base <- "full"  # Base level for comparison between models
res_base <- "uv"  # Base level for comparison between responses

dat <- readRDS(here("out", "inferential", atlas_nm, input_name))
dat <- bind_rows(dat, .id = "model__response__session") %>%
  separate(model__response__session, c("model", "response", "session"), sep = "__") %>%
  filter(session %in% .env$test_session) %>%
  arrange(model, response, session, region)

responses <- unique(dat$response)
models <- unique(dat$model)

mdl_diff_dat <- dat %>%
  filter(Term == "TRR") %>%
  select(Estimate, region, model, response, session) %>%
  pivot_wider(values_from = Estimate, names_from = model,
    id_cols = c(region, response, session))
for (mdl in models) {
  if (mdl != mdl_base) mdl_diff_dat[[paste(mdl, "-", mdl_base)]] <- (
    mdl_diff_dat[[mdl]] - mdl_diff_dat[[mdl_base]])
}

res_diff_dat <- dat %>%
  filter(Term == "TRR") %>%
  select(Estimate, region, model, response, session) %>%
  pivot_wider(values_from = Estimate, names_from = response,
    id_cols = c(region, model, session))
for (res in responses) {
  if (res != res_base) res_diff_dat[[paste(res, "-", res_base)]] <- (
    res_diff_dat[[res]] - res_diff_dat[[res_base]])
}
```

# Model Comparison

## Fitting Statistics

First we compare the Bayesian R-squared of the models (see [here](https://paul-buerkner.github.io/brms/reference/bayes_R2.brmsfit.html)). The higher the value, the better the fit. We can see that generally, `no_lscov_symm` > `no_lscov` > `full`.

```{r, Bayes-R2, echo = FALSE, message = FALSE, fig.dim = c(16, 12)}
for (i in seq_along(responses)) {
  res <- responses[[i]]
  fig <- ggplot(dat %>% filter(Term == "Bayes_R2", response == .env$res),
    aes(x = model, y = Estimate, group = region)) +
    geom_line(linetype = 1, color = "black", size = 0.5) + geom_point(size = 0.5) +
    labs(x = NULL, y = NULL, title = res) + ylim(0, 0.05) +
    theme(axis.text.x = element_text(size = 14, hjust = 1, vjust = 1, angle = 25))
  if (i == 1) figs <- fig + labs(y = "Bayes R2") else figs <- figs + fig
}
figs + plot_annotation(title = "Bayesian R-Squared for Core32 ROIs", tag_levels = "A")
```

```{r, elpd-waic, echo = FALSE, message = FALSE, fig.dim = c(16, 12), eval = FALSE}
for (i in seq_along(responses)) {
  res <- responses[[i]]
  fig <- ggplot(dat %>% filter(response == .env$res, Term == "elpd_waic"),
    aes(x = model, y = Estimate, group = region)) +
    geom_line(linetype = 1, color = "black") + geom_point() +
    labs(x = NULL, y = NULL, title = res) +
    theme(axis.text.x = element_text(size = 14, hjust = 1, vjust = 1, angle = 25))
  if (i == 1) figs <- fig + labs(y = "elpd_waic") else figs <- figs + fig
}
figs + plot_annotation(title = "elpd_waic for Core32 ROIs",
  tag_levels = "A")
```

Here we show the difference in the expected log predictive density (elpd) estimated by leave-one-out (loo) cross-validation across three models (see [here](https://mc-stan.org/loo/reference/loo.html)). The higher the value, the better the fit. Here `no_lscov_symm` is used as the base level. We can see that all values are negative, indicating that `no_lscov_symm` is indeed the best fit. Results using [`waic`](https://mc-stan.org/loo/reference/waic.html) instead of `loo` are almost identical.

```{r, diff-elpd-loo, echo = FALSE, message = FALSE, fig.dim = c(16, 12)}
diff_dat <- get_diff_dat(dat %>% filter(Term == "elpd_loo"), name_term = "model",
  base_level = "no_lscov_symm", val_term = "Estimate",
  id_term = c("region", "session", "response"))
for (i in seq_along(responses)) {
  res <- responses[[i]]
  fig <- ggplot(diff_dat %>% filter(response == .env$res),
    aes(x = model, y = Estimate, group = region)) +
    geom_line(linetype = 1, color = "black") + geom_point() +
    labs(x = NULL, y = NULL, title = res) + ylim(-20, 0) +
    theme(axis.text.x = element_text(size = 14, hjust = 1, vjust = 1, angle = 25))
  if (i == 1) figs <- fig + labs(y = "Difference between EPLD_LOO") else figs <- figs + fig
}
figs + plot_annotation(title = 
  "Difference in ELPD_LOO between complex models and simplest model for Core32 ROIs",
  tag_levels = "A")
```

## Test-Retest Reliability

Here we compare the test-retest reliability (TRR) of high-low control demand contrast among response variables `r responses` and models `r models`:

```{r, TRR-brain, echo = FALSE, message = FALSE, fig.dim = c(16, 9)}
for (i in seq_along(responses)) {
  for (j in seq_along(models)) {
    res <- responses[[i]]
    mdl <- models[[j]]
    fig <- brain_plot(
      dat %>% filter(Term == "TRR", response == .env$res, model == .env$mdl),
      lim = c(0, 1), fig_title = paste(res, mdl, sep = ", ")
    )
    if (j == 1) fig_row <- fig else fig_row <- fig_row + fig
  }
  if (i == 1) figs <- fig_row else figs <- figs / fig_row
}
figs + plot_annotation(title = "Test-Retest Reliability", tag_levels = "A")
```

Here we plot the difference in TRR between different models:

```{r, TRR-mdl-diff-brain, echo = FALSE, message = FALSE, fig.dim = c(16, 9)}
first_row <- TRUE
for (i in seq_along(models)) {
  mdl <- models[[i]]
  if (mdl == mdl_base) next
  for (j in seq_along(responses)) {
    res <- responses[[j]]
    fig <- brain_plot(mdl_diff_dat %>% filter(response == .env$res),
      stat_term = paste(mdl, "-", mdl_base), lim = c(-0.2, 0.2),
      fig_title = paste0(mdl, " - ", mdl_base, ", ", res))
    if (j == 1) fig_row <- fig else fig_row <- fig_row + fig
  }
  if (first_row) {
    first_row <- FALSE
    figs <- fig_row
  } else figs <- figs / fig_row
}
figs + plot_annotation(title = "Difference in TRR between models", tag_levels = "A")
```

and between different response variables:

```{r, TRR-res-diff-brain, echo = FALSE, message = FALSE, fig.dim = c(16, 9)}
first_row <- TRUE
for (i in seq_along(responses)) {
  res <- responses[[i]]
  if (res == res_base) next
  for (j in seq_along(models)) {
    mdl <- models[[j]]
    fig <- brain_plot(res_diff_dat %>% filter(model == .env$mdl),
      stat_term = paste(res, "-", res_base), lim = c(-0.2, 0.2),
      fig_title = paste0(res, " - ", res_base, ", ", mdl))
    if (j == 1) fig_row <- fig else fig_row <- fig_row + fig
  }
  if (first_row) {
    first_row <- FALSE
    figs <- fig_row
  } else figs <- figs / fig_row
}
figs + plot_annotation(title = "Difference in TRR between responses", tag_levels = "A")
```

Here we plot the distribution of TRR over the core32 parcels.

```{r, TRR-hist, echo = FALSE, message = FALSE, fig.dim = c(16, 9)}
for (i in seq_along(responses)) {
  res <- responses[[i]]
  fig <- ggplot(dat %>% filter(Term == "TRR", response == .env$res),
    aes(x = model, y = Estimate, fill = model)) +
    geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
    geom_jitter(size = 0.02, height = 0, width = 0.1) +
    ylim(-0.5, 1) + labs(x = res, y = NULL) +
    theme(axis.text.x = element_blank())
  if (i == 1) figs <- fig + labs(y = "TRR") else figs <- figs + fig
}
figs + plot_layout(guides = "collect") +
  plot_annotation(title = "Test-Retest Reliability over ROIs", tag_levels = "A")
```

Here we plot the TRR over core32 parcels and compare it with the old models we fit, which use either `mda::fda()`("ridge") or `sparsediscrim::lda_schafer()`("rda_full" and "rda_diag") to generate the response variable. The TRR model is similar to `"no_lscov_symm"` model but with only a fixed sigma term. Note that here the figures are plotted for each TRR model while each violin plot are for one response type.

```{r, TRR-hist-core32, echo = FALSE, message = FALSE, fig.dim = c(21, 9)}

# Read fixed_std model stats
old_dat <- readRDS(here("out", "spatial", "archive", "fix_std_mdl_core32_stats.rds"))
old_dat <- bind_rows(old_dat, .id = "model__response__session") %>%
  separate(model__response__session, c("model", "response", "session"), sep = "__") %>%
  filter(Term == "TRR")
full_dat <- bind_rows(dat, old_dat)
full_responses <- unique(full_dat$response)
full_models <- unique(full_dat$model)

for (i in seq_along(full_models)) {
  mdl <- full_models[[i]]
  fig <- ggplot(full_dat %>% filter(Term == "TRR", model == .env$mdl),
    aes(x = response, y = Estimate, fill = response)) +
    geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
    geom_jitter(size = 0.02, height = 0, width = 0.1) +
    ylim(-0.5, 1) + labs(x = mdl, y = NULL) +
    theme(axis.text.x = element_blank())
  if (i == 1) figs <- fig + labs(y = "TRR") else figs <- figs + fig
}
figs + plot_layout(nrow = 1, guides = "auto") +
  plot_annotation(title = "Test-Retest Reliability over ROIs", tag_levels = "A")
```

And the distribution of the difference between response types:

```{r, TRR-diff-hist-core32, echo = FALSE, message = FALSE, fig.dim = c(21, 9)}
old_responses <- unique(old_dat$response)
res_diff_old_dat <- old_dat %>%
  filter(Term == "TRR") %>%
  select(Estimate, region, model, response, session) %>%
  pivot_wider(values_from = Estimate, names_from = response,
    id_cols = c(region, model, session))
for (res in old_responses) {
  if (res != res_base) res_diff_old_dat[[paste(res, "-", res_base)]] <- (
    res_diff_old_dat[[res]] - res_diff_old_dat[[res_base]])
}
res_diff_old_long <- res_diff_old_dat %>%
  select(region, model, session, contains(" ")) %>%
  pivot_longer(contains(" "), names_to = "Term", values_to = "Estimate")
res_diff_long <- res_diff_dat %>%
  select(region, model, session, contains(" ")) %>%
  pivot_longer(contains(" "), names_to = "Term", values_to = "Estimate")
res_diff_full <- bind_rows(res_diff_long, res_diff_old_long)

for (i in seq_along(full_models)) {
  mdl <- full_models[[i]]
  fig <- ggplot(res_diff_full %>% filter(model == .env$mdl),
    aes(x = Term, y = Estimate, fill = Term)) +
    geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
    geom_jitter(size = 0.02, height = 0, width = 0.1) +
    ylim(-1, 1) + labs(x = mdl, y = NULL) +
    theme(axis.text.x = element_blank())
  if (i == 1) figs <- fig + labs(y = "TRR Difference") else figs <- figs + fig
}
figs + plot_layout(nrow = 1, guides = "auto") +
  plot_annotation(title = "Difference of TRR between response variables over ROIs", tag_levels = "A")
```